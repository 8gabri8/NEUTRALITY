{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579b567a-2b80-4bf1-8cc1-c514da76ca50",
   "metadata": {},
   "source": [
    "# (ADA) Homework 1: Scoring the Language Model Olympics\n",
    "\n",
    "---\n",
    "\n",
    "By the end of this homework, we expect you to be able to:\n",
    "\n",
    "- Load data and handle data using pandas;\n",
    "- Navigate the documentation of Python packages by yourself;\n",
    "- Filter and tidy up noisy real-world datasets;\n",
    "- Aggregate your data in different (and hopefully helpful) ways;\n",
    "- Create meaningful visualizations to analyze the data;\n",
    "- Communicate your findings in a clear and concise manner\n",
    "\n",
    "---\n",
    "\n",
    "**Important Dates.**\n",
    "\n",
    "- Homework release: Fri 04 Oct 2024\n",
    "- Homework due: Sat 18 Oct 2024, 23:59\n",
    "- Grade release: Mon 04 Nov 2024\n",
    "\n",
    "**Some rules**\n",
    "\n",
    "- You are allowed to use any built-in Python library that comes with Anaconda. If you want to use an external library, you may do so, but must justify your choice.\n",
    "- Make sure you use the data folder provided in the repository in read-only mode. (Or alternatively, be sure you don‚Äôt change any of the files.)\n",
    "- Be sure to provide a concise textual description of your thought process, the assumptions you made, the solution you implemented, and explanations for your answers. A notebook that only has code cells will not suffice. To avoid confusion: use short comments for longer code answers.\n",
    "- For questions containing the /Discuss:/ prefix, answer not with code, but with a textual explanation (in markdown).\n",
    "- Back up any hypotheses and claims with data, since this is an important aspect of the course.\n",
    "- Please write all your comments in English, and use meaningful variable names in your code. Your repo should have a single notebook (plus the required data files) in the master/main branch. If there are multiple notebooks present, we will not grade anything.\n",
    "- We will not run your notebook for you! Rather, we will grade it as is, which means that only the results contained in your evaluated code cells will be considered, and we will not see the results in unevaluated code cells. Thus, be sure to hand in a fully-run and evaluated notebook. In order to check whether everything looks as intended, you can check the rendered notebook on the GitHub website once you have pushed your solution there.\n",
    "- In continuation to the previous point, interactive plots, such as those generated using the ‚Äòplotly‚Äô package, should be strictly avoided! Make sure to print results and/or dataframes that confirm you have properly addressed the task.\n",
    "\n",
    "**A Note on using Language Models (LMs)**\n",
    "\n",
    "If you try hard enough, you will likely get away with cheating. Fortunately, our job is not to police, but rather to educate! So, please consider the following:\n",
    "- Presumably, you are taking this course to learn something! LMs are not always right ([they often fail in silly ways](https://community.openai.com/t/why-9-11-is-larger-than-9-9-incredible/869824/4)). This course should prepare you to detect when they are wrong!\n",
    "- Some of the TAs on this course literally published many works on detecting machine-generated text.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f85896e-c0ae-4ae3-af41-46149faa2278",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "Context\n",
    "AI is booming! Newspapers, influencers, and your relatives all agree that AI is important. But while almost everyone agrees that AI is the future, much is unclear about what that future looks like‚Ä¶\n",
    "\n",
    "Freshly graduated from the EPFL, you are hired by the Swiss government to advise on a large-scale ‚ÄúAI integration‚Äù initiative code-named **\"NEUTRALITY\"** (Navigating Efficient Upgrades Through Robust Artificial Learning Integration Techniques Yearly). Convinced by the stunning progress in language modeling, the government would like to battle the growing shortages in the education sector by using LMs. Your job description: investigate which LMs might be best suited!\n",
    "\n",
    "You are given the results of three LMs on the [‚ÄúMassive Multitask Language Understanding (MMLU)‚Äù](https://arxiv.org/abs/2009.03300) dataset to compare. This famous dataset consists of 57 subjects with multiple-choice questions, covering diverse subjects like mathematics, computer science, history, and law. Most providers of state-of-the-art LMs use this dataset to showcase the versatility of their latest models. Unfortunately, Horta-Ribeiro, the intern responsible for collecting the results, didn‚Äôt take EPFL‚Äôs famous ADA course. As a result, the collected datasets are slightly corrupted.\n",
    "\n",
    "### A very brief primer on Language Models\n",
    "Language models (LMs) are sophisticated statistical models designed to understand and generate human-like text. At their core, LMs are trained to predict the most likely continuation of a given input text. For example, given the input \"The cat sat on the,\" an LM might predict \"mat\" as a likely continuation.\n",
    "LMs are trained on vast text samples from various sources, including books, websites, and social media. This extensive training allows them to capture patterns and relationships in language, enabling them to generate coherent and contextually appropriate text across a wide range of topics and styles.\n",
    "\n",
    "While LMs can produce text that appears to be written by intelligent humans, it's important to note that their capabilities can diverge from human intelligence in unexpected ways. They may sometimes generate factually incorrect information or struggle with complex reasoning tasks.\n",
    "\n",
    "Two key concepts in understanding LMs are:\n",
    "1. **Tokens**: LMs process text using \"tokens\" rather than individual characters. Tokens can be words, parts of words, or punctuation marks. For example, the sentence \"I love AI!\" might be tokenized as [\"I\", \"love\", \"AI\", \"!\"]. Tokenization is the first step in both training and using an LM.\n",
    "2. **Context**: The input text provided to an LM is called the \"context.\" This context informs the model's predictions or generations. A longer or more specific context often leads to more accurate and relevant outputs.\n",
    "\n",
    "[See: Wikipedia entry on language models](https://en.wikipedia.org/wiki/Large_language_model)\n",
    "\n",
    "###  Files for this assignment\n",
    "This assignment is divided into three tasks, each of which should bring you a step closer to providing a recommendation toward project NEUTRALITY‚Äôs objectives:\n",
    "\n",
    "- **Task 1**: Inspecting the results and getting your first model ranking\n",
    "- **Task 2**: Inspecting the underlying data used to generate the results for possible biases\n",
    "- **Task 3**: Learning about tokens and providing a final recommendation\n",
    "\n",
    "\n",
    "```\n",
    "üìÅ PROJECT_NEUTRALITY\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ üìÑ analysis.ipynb (the file you're currently reading!)\n",
    "‚îú‚îÄ‚îÄ üìÑ requirements.txt (install into your environment)\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ üìÅ task_1\n",
    "‚îú‚îÄ‚îÄ üìÅ task_2\n",
    "‚îî‚îÄ‚îÄ üìÅ task_2.5\n",
    "```   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "95ce4c12-9681-401e-9489-aa0765b19d5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# please make sure you install the packages listed in the requirements.txt file in your environment!\n",
    "# using pip\n",
    "# pip install -r requirements.txt\n",
    "#\n",
    "# using Conda:\n",
    "# conda create --name <env_name> --file requirements.txt\n",
    "#\n",
    "# some basic imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from scipy.stats import ttest_ind\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62594ad-4f5f-4a46-80bc-deacf66b62e9",
   "metadata": {},
   "source": [
    "## Task 1 (18 points): What's in an average anyway?\n",
    "\n",
    "The files needed to complete task 1 can be found in the folder \"`data/task_1/`:\n",
    "```\n",
    "task_1/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ mmlu_data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test.csv\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ lm_scores/\n",
    "    ‚îú‚îÄ‚îÄ lm_X.csv\n",
    "    ‚îú‚îÄ‚îÄ lm_Y.csv\n",
    "    ‚îî‚îÄ‚îÄ lm_Z.csv\n",
    "```\n",
    "\n",
    "We will start by loading, (manually) inspecting, and cleaning the data. Although it doesn't seem \"glamorous\" (nor is it particularly fun...) - manually inspecting data is extremely important! In fact, it's one of the few things most AI and Data Science researchers agree on :). Next, we will take a first pass on ordering our Olympic podium between three LMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8605646-79fa-4cb3-8137-b6951bd1e064",
   "metadata": {},
   "source": [
    "### 1.1 (1 pt)\n",
    " \n",
    "Load the subfiles contained in the `mmlu_data` and `lm_scores` folders into separate dataframes:\n",
    "- `df_test`\n",
    "- `df_x`\n",
    "- `df_y`\n",
    "- `df_z`\n",
    "\n",
    "for each, print their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce5e96-7de6-463d-a00b-6a4b2cfc8cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets into separate dataframes\n",
    "df_test = pd.read_csv('task_1/mmlu_data/test.csv')\n",
    "df_x = pd.read_csv('task_1/lm_scores/lm_X.csv')\n",
    "df_y = pd.read_csv('task_1/lm_scores/lm_Y.csv')\n",
    "df_z = pd.read_csv('task_1/lm_scores/lm_Z.csv')\n",
    "\n",
    "# Print the size of each dataframe \n",
    "print(f\"Size of df_test: {df_test.size}\")\n",
    "print(f\"Size of df_x: {df_x.size}\")\n",
    "print(f\"Size of df_y: {df_y.size}\")\n",
    "print(f\"Size of df_z: {df_z.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbda57d-7df2-4e33-b31c-52bc0af6753e",
   "metadata": {},
   "source": [
    "### 1.2 (4 pt)\n",
    "Unfortunately, LMs don't always output the format we want. In the column `result`, the value should be one of A, B, C, or D. \n",
    "\n",
    "A. For each of the LM score dataframes, use a `value_counts()` operation and print the results. \n",
    "\n",
    "B. /Discuss:/ Inspect the results and describe the types of answer formats you see. Besides the \"expected\" case, you should be able to find at least four unexpected formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0abfe-680b-46f0-9d4f-e9a681a2155f",
   "metadata": {},
   "source": [
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79936f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the value_counts operation on the 'result' column to count the number of \n",
    "# different answer for each lm\n",
    "\n",
    "# We use .apply(repr) to display the hidden characters\n",
    "\n",
    "print(df_x['result'].apply(repr).value_counts()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_y['result'].apply(repr).value_counts()[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11119f75-00e5-498f-a3dc-b60d9a56f696",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_z['result'].apply(repr).value_counts()[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb933e84-2161-4144-8a5e-fd36cac1a6bd",
   "metadata": {},
   "source": [
    "## B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3205b584-d165-49e9-b322-77ea34bf024e",
   "metadata": {},
   "source": [
    "The expected case would be the LMs answering A, B, C or D.\n",
    "\n",
    "The first observation is that the same letters A, B, C or D appear multiple times in the value_count. To investigate this, we used the repr() function to show the hidden characters and we conclude now that this is due to trailing spaces added after the answers by the LMs.\n",
    "\n",
    "We report 6 unexpected format among the most frequent answers : \n",
    "\n",
    "**1 . Answer : A, B, C or D :**  The LMs use the prefix 'Answer :' in front of the letters.\n",
    "\n",
    "**2. Not Sure :** The LMs don't return an answer at all and instead return 'Not sure'.\n",
    "\n",
    "**3. nan :** The DataSet contains missing or undefined values.\n",
    "\n",
    "**4. None of the above :** The LMs sometimes think that there is no correct answer in the proposed choices.\n",
    "\n",
    "**5. Sentence to explaint the answer :** The LMs give a full sentence to show the reasoning that led to the answer.\n",
    "\n",
    "**6. Stating the answer :** The LMs state the answer before giving the answer letter. \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5551d-1968-427b-bdd6-51d996898e7c",
   "metadata": {},
   "source": [
    "### 1.3 (5 pt)\n",
    "Oh oh... That doesn't look great. Simply dropping all invalid answers seems overly wasteful, yet fixing all of these looks like a mess! Instead, let's focus for now on fixing just those answers of length < 10 characters that require only a single `str.replace()` operation. \n",
    "\n",
    "For example, if the answer looks like `--A--`, we could fix this by using the following simple function:\n",
    "\n",
    "```\n",
    "def clean_answer(s, pattern='-'):\n",
    "    return str(s).replace(pattern, '')\n",
    "\n",
    "dirty_answer = '--A--'\n",
    "clean_answer = clean_answer(dirty_answer)\n",
    "```\n",
    "\n",
    "A. Filter the three score dataframes to include only answers with less than 10 characters. Make a deep copy of the dataframes as you filter them.\n",
    "\n",
    "B. Modify the `clean_answer()` example function to clean the answers in the filtered data frames using the `apply()` functionality. Finally, make sure **all remaining answers are one of `A, B, C, or D`.**\n",
    "\n",
    "C. /Discuss:/ Compare the sizes of the original and filtered data frames. What do you see? Why might this be a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8da26-5c08-4cf1-b38b-c29477bfb258",
   "metadata": {},
   "source": [
    "### A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aef1f933-20bf-426a-ac9d-a35e273b9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we filter each df to include only answers with less than 10 chars and make a deep copy of each df\n",
    "df_x_preprocessed = df_x[df_x[\"result\"].str.len() < 10].copy()\n",
    "df_y_preprocessed = df_y[df_y[\"result\"].str.len() < 10].copy()\n",
    "df_z_preprocessed = df_z[df_z[\"result\"].str.len() < 10].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f132ce-1d6e-40a0-9615-b8528e9746bf",
   "metadata": {},
   "source": [
    "### B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54f296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We redefine the function and change the pattern argument to a list\n",
    "def clean_answer(s, pattern=[]):\n",
    "    for p in pattern:\n",
    "        s = str(s).replace(p, '')\n",
    "    return s\n",
    "\n",
    "# We create variables to perform the cleaning operation afterwards\n",
    "dfs = [('x', df_x_preprocessed), ('y', df_y_preprocessed), ('z', df_z_preprocessed)]\n",
    "patterns = [' ', 'Answer:']\n",
    "accepted_answers = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "# We use a loop to clean each df and check that it is well preprocessed\n",
    "for name, df in dfs:\n",
    "    df[\"result\"] = df[\"result\"].apply(lambda x: clean_answer(x, patterns))\n",
    "    \n",
    "    df.drop(df[~df[\"result\"].isin(accepted_answers)].index, axis=0, inplace=True)\n",
    "\n",
    "    # We reset the indexes after the drop operation\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # We check that all results are accepted\n",
    "    all_accepted = df[\"result\"].isin(accepted_answers).all()\n",
    "    print(f\"All results in LLM {name} are accepted: {all_accepted}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415cd9b",
   "metadata": {},
   "source": [
    "### C : Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3555ae0-4409-4a26-b93c-772a5f4b02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 10\n",
    "\n",
    "for model_name, df, df_preprocessed in [('x', df_x, df_x_preprocessed), ('y', df_y, df_y_preprocessed), ('z', df_z, df_z_preprocessed)]:\n",
    "    before = df.shape[0]\n",
    "    after = df_preprocessed.shape[0]\n",
    "    loss_percentage = ((before - after) / before) * 100\n",
    "    print(f\"LLM {model_name}\\n\\tNumber of answers with less than {max_length} characters:\\n\\t\\tBefore: {before}\\n\\t\\tAfter: {after}\\n\\t\\tLoss Percentage: {loss_percentage:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff4361c-2da1-4b46-8bcf-3e4885ca1f5a",
   "metadata": {},
   "source": [
    "We see that the sizes of the pre-processed DFs are not too different from the original sizes. This might be a problem because some of the unique or unexpected answers that the LMs could have generated are no longer part of the dataset.\n",
    "\n",
    "This could introduce bias into the data with the loss of valuable and diverse data. This type of answers can be beneficial for analyzing the LMs and improving them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1cf129-09dd-47b1-9737-2c4d57eb8853",
   "metadata": {},
   "source": [
    "### 1.4 (3 pt)\n",
    "\n",
    "Now that our answer columns are nicely formatted, let's take a look at model performance:\n",
    "\n",
    "A. Both the `MMLU` dataframes and the language model score data frames have the columns `question_id`. For each of the language model score data frames, use an inner join operation with the `df_test` dataframe on the `question_id` column.\n",
    "\n",
    "B. Add a new column to each of the resulting dataframes called `correct`, that checks if the model's answer in `result` is the same as the expected answer in the column `answer`. Then, print the average score of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab459b56-781e-4581-a9c0-28e91b5eec18",
   "metadata": {},
   "source": [
    "## A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1226453-dbf7-42a8-b447-c10b63573c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the DFs with an inner join operation\n",
    "df_x_merged = df_x_preprocessed.merge(df_test, on='question_id', how='left')\n",
    "df_y_merged = df_y_preprocessed.merge(df_test, on='question_id', how='left')\n",
    "df_z_merged = df_z_preprocessed.merge(df_test, on='question_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10343394-0367-46c5-bd4e-f2de08e4d1e5",
   "metadata": {},
   "source": [
    "# B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50b024d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column 'correct' of boolean values \n",
    "df_x_merged['correct'] = df_x_merged['result'] == df_x_merged['answer']\n",
    "df_y_merged['correct'] = df_y_merged['result'] == df_y_merged['answer']\n",
    "df_z_merged['correct'] = df_z_merged['result'] == df_z_merged['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518cf3c-29dc-4fc6-af75-856fee97727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the avg score for each model by dividing the number of TRUE in the 'correct' column \n",
    "# by the number of rows\n",
    "\n",
    "avg_score_x = df_x_merged['correct'].sum() / df_x_merged.shape[0] * 100\n",
    "print(f\"The average score for the LM X is: {avg_score_x:.2f}%\")\n",
    "\n",
    "avg_score_y = df_y_merged['correct'].sum() / df_y_merged.shape[0] * 100\n",
    "print(f\"The average score for the LM Y is: {avg_score_y:.2f}%\")\n",
    "\n",
    "avg_score_z = df_z_merged['correct'].sum() / df_z_merged.shape[0] * 100\n",
    "print(f\"The average score for the LM Z is: {avg_score_z:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69decfd8-8083-4c2f-8263-a153d55efede",
   "metadata": {},
   "source": [
    "### 1.5 (5 pt)\n",
    "\n",
    "Hmmm, something doesn't seem quite right. Let's investigate how \"balanced\" this dataset is:\n",
    "\n",
    "A. For each of the 57 subjects in the MMLU, compare the number of questions answered by each model. Print the subjects for which there is a more than 10% difference.\n",
    "\n",
    "B. Propose and implement a reasonable way to rebalance the results. (e.g., while throwing away 100% of the results perfectly rebalances the results, it is not reasonable).\n",
    "\n",
    "C. Finally, print the updated accuracy on the rebalanced data.\n",
    "\n",
    "**hint:**:\n",
    "- (A) For a given subject, let model X and model Y have answered 181 and 200 questions respectively. You can consider this a 10% difference from the perspective of X since: (200 - 181) / 181 > 0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f480762b-4da0-4428-8c70-480c2180bc96",
   "metadata": {},
   "source": [
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a728bb-de66-4a47-9bb0-7d7f6f916a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create DFs that has the number of questions for each category\n",
    "counts_cat_x = df_x_merged.groupby(\"subject\")[\"subject\"].count()\n",
    "counts_cat_y = df_y_merged.groupby(\"subject\")[\"subject\"].count()\n",
    "counts_cat_z = df_z_merged.groupby(\"subject\")[\"subject\"].count()\n",
    "\n",
    "# We combine them into a single dataframe for a more easy comparison\n",
    "counts_combined = pd.DataFrame({'X': counts_cat_x,'Y': counts_cat_y,'Z': counts_cat_z}).reset_index()\n",
    "\n",
    "# We create a list to store the subjects with more than 10% diff\n",
    "subjects = []\n",
    "\n",
    "# We iterate through the DF and compare the counts\n",
    "for index, data in counts_combined.iterrows():\n",
    "    subject = data['subject']\n",
    "    x_count = data['X']\n",
    "    y_count = data['Y']\n",
    "    z_count = data['Z']\n",
    "\n",
    "    # We check X and Y\n",
    "    if abs(y_count - x_count) / x_count > 0.10:\n",
    "        subjects.append(subject)\n",
    "    \n",
    "    # We check X and Z\n",
    "    if abs(z_count - x_count) / x_count > 0.10:\n",
    "        subjects.append(subject)\n",
    "    \n",
    "    # We check Y and Z\n",
    "    if abs(z_count - y_count) / y_count > 0.10:\n",
    "        subjects.append(subject)\n",
    "\n",
    "# We delete duplicate from the list\n",
    "subjects = list(set(subjects))\n",
    "\n",
    "for s in subjects:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7e479-cfc1-4312-a839-3232439c0234",
   "metadata": {},
   "source": [
    "## B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f155ee-877d-4868-8ecf-840f0ecafbea",
   "metadata": {},
   "source": [
    "We can use ***downsampling*** : \n",
    "\n",
    "we randomly remove questions from the subjects with more than 10% difference so that the 3 LMs have the same number of questions in these subjects.\n",
    "\n",
    "To do so, we identify for each subject with more than 10%, the minimum number of questions in the 3 LMs, and in the remaining 2 we drop questions to match the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0af454c4-dae7-4b30-9fe6-7bc503016bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    # We get the count of questions for each lm for this particular subject\n",
    "    x_count = counts_cat_x.loc[subject]\n",
    "    y_count = counts_cat_y.loc[subject]\n",
    "    z_count = counts_cat_z.loc[subject]\n",
    "    \n",
    "    # We find the min across the 3 LMs\n",
    "    min_count = min(x_count, y_count, z_count)\n",
    "    \n",
    "    # We downsample lm x if it has more than min_count questions\n",
    "    if x_count > min_count:\n",
    "        df_x_merged = df_x_merged.drop(\n",
    "            df_x_merged[df_x_merged['subject'] == subject].sample(n=(x_count - min_count)).index\n",
    "        )\n",
    "\n",
    "    # We downsample lm y if it has more than min_count questions\n",
    "    if y_count > min_count:\n",
    "        df_y_merged = df_y_merged.drop(\n",
    "            df_y_merged[df_y_merged['subject'] == subject].sample(n=(y_count - min_count)).index\n",
    "        )\n",
    "        \n",
    "    # We downsample lm z if it has more than min_count questions\n",
    "    if z_count > min_count:\n",
    "        df_z_merged = df_z_merged.drop(\n",
    "            df_z_merged[df_z_merged['subject'] == subject].sample(n=(z_count - min_count)).index\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbe07f-1390-4378-96af-421326d75f2f",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd2eca-45ad-41f1-bb7c-20c8a7ad76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print the avg accuracy for each lM after downsampling\n",
    "\n",
    "avg_score_x = df_x_merged['correct'].sum() / df_x_merged.shape[0] * 100\n",
    "print(f\"The average score for the LM X is: {avg_score_x:.2f}%\")\n",
    "\n",
    "avg_score_y = df_y_merged['correct'].sum() / df_y_merged.shape[0] * 100\n",
    "print(f\"The average score for the LM Y is: {avg_score_y:.2f}%\")\n",
    "\n",
    "avg_score_z = df_z_merged['correct'].sum() / df_z_merged.shape[0] * 100\n",
    "print(f\"The average score for the LM Z is: {avg_score_z:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b2f61-0529-4b6d-a3a7-af786a4d79ae",
   "metadata": {},
   "source": [
    "## Task 2 (26 points): What do you mean A > D > B > C...?\n",
    "\n",
    "Nice work! Having successfully inspected, cleaned, and rebalanced the provided data, you head over to director of the government's NEUTRALITY project. Ms. Sakota is happy with your work so far, but worried that the sloppy intern might have done more undetected damage. To be sure, she orders a new set of evaluations of all models on both MMLU and another dataset.\n",
    "\n",
    "After cleaning up and rebalancing, you are left with the concatenated score files in the second folder `task_2`:\n",
    "```\n",
    "task_2/\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ lm_scores_mmlu.csv\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ lm_scores_other.csv\n",
    "```\n",
    "\n",
    "Each has a new column called `model_name`, which is one of `X, Y` or `Z`.\n",
    "\n",
    "\n",
    "\n",
    "_NOTE: **only** use data from `task_2` and `task_2_5` for this assignment! The values in `lm_scores_mmlu.csv` will NOT be the same as the dataframes you finished in task 1. This is due to \"randomness\" or \"temperature\" in language model inference. This can slightly shift around generative results. (Conveniently: it also ensures any mistakes made in Task 1 don't propogate further ;) )_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a889a76b-e034-4d2f-929e-0ef1f250a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROVIDED CODE\n",
    "df_mmlu = pd.read_csv('task_2/lm_scores_mmlu.csv')\n",
    "df_other = pd.read_csv('task_2/lm_scores_other.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31edde3-cc91-4d08-81c7-b5e98cf0ff9c",
   "metadata": {},
   "source": [
    "### 2.1 (4 pt)\n",
    "\n",
    "Let's explore the new results:\n",
    "\n",
    "A. Compute the mean accuracy and standard errors of each model on both datasets and print the results.\n",
    "\n",
    "B. Then, show your results in a bar plot using standard errors with a 95% confidence interval around the mean. Make sure the plot is easy to read and well annotated.\n",
    "\n",
    "C. /Discuss:/ the plot you created: (i) can you say that one of the models is the best? (ii) is there anything that seems odd?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe8cca-acd1-4f5e-a938-e8d3a850f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "df_accuracy = pd.DataFrame()\n",
    "\n",
    "for model in df_mmlu[\"model_name\"].unique():\n",
    "    # MMLU Dataset\n",
    "    df_filtered = df_mmlu[df_mmlu[\"model_name\"] == model]\n",
    "    df_temp = pd.DataFrame({\n",
    "        \"accuracy\": [df_filtered[\"correct\"].sum()/df_filtered.shape[0]],\n",
    "        \"sem\": [df_filtered[\"correct\"].sem()],\n",
    "        \"model\": [model],\n",
    "        \"dataset\": [\"mmlu\"]\n",
    "    })\n",
    "    df_accuracy = pd.concat([df_accuracy, df_temp], ignore_index=True)\n",
    "\n",
    "    # Other Dataset\n",
    "    df_filtered = df_other[df_other[\"model_name\"] == model]\n",
    "    df_temp = pd.DataFrame({\n",
    "        \"accuracy\": [df_filtered[\"correct\"].sum()/df_filtered.shape[0]],\n",
    "        \"sem\": [df_filtered[\"correct\"].sem()],\n",
    "        \"model\": [model],\n",
    "        \"dataset\": [\"other\"]\n",
    "    })\n",
    "    df_accuracy = pd.concat([df_accuracy, df_temp], ignore_index=True)\n",
    "\n",
    "# Calculate 95% confidence intervals (the height of the error bar in the next plot)\n",
    "df_accuracy[\"yerr\"] = 1.96 * df_accuracy[\"sem\"]\n",
    "\n",
    "\n",
    "display(df_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a176c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B\n",
    "fig, axs = plt.subplots(1,2,figsize=(15,5), sharey=True)\n",
    "\n",
    "#mmlu dataset\n",
    "mmlu_data = df_accuracy[df_accuracy[\"dataset\"] == \"mmlu\"]\n",
    "axs[0].bar(mmlu_data[\"model\"], mmlu_data[\"accuracy\"], yerr=mmlu_data[\"yerr\"], capsize=5, color='skyblue')\n",
    "axs[0].set_title('Mean Accuracy of Models - MMLU Dataset')\n",
    "axs[0].set_ylabel('Mean Accuracy')\n",
    "axs[0].set_xlabel('Model Name')\n",
    "axs[0].set_xticks(mmlu_data[\"model\"])\n",
    "axs[0].set_xticklabels(mmlu_data[\"model\"])\n",
    "axs[0].grid(axis='y')\n",
    "\n",
    "# Bar plot for df_other\n",
    "other_data = df_accuracy[df_accuracy[\"dataset\"] == \"other\"]\n",
    "axs[1].bar(other_data[\"model\"], other_data[\"accuracy\"], yerr=other_data[\"yerr\"], capsize=5, color='lightgreen')\n",
    "axs[1].set_title('Mean Accuracy of Models - Other Dataset')\n",
    "axs[1].set_xlabel('Model Name')\n",
    "axs[1].set_xticks(other_data[\"model\"])\n",
    "axs[1].set_xticklabels(other_data[\"model\"])\n",
    "axs[1].grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c9fbfe",
   "metadata": {},
   "source": [
    "C. /Discuss:/\n",
    "\n",
    "Simply looking at the charts, model X seems to be performing better than the other 2 when generalizang to other datasets, and model Z doing the worst by quite a margin. However this is not enough to conclude anything about the quality of the models or whcih one is really the best.\n",
    "One observation we can make is that the models seem to be performing quite differently from one dataset to the next, with a difference in accuracy of about 5% between the two datasets for models X and Y. Something else we can notice is that the standard errors are ??significantly?? bigger in the \"Other Dataset\" compared to the \"MMLU Dataset\".\n",
    "CI semmes to be very samll\n",
    "\n",
    "strange is thta th emodles seems ot better differelty in the 2 satsasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0ec62-f2d1-4bae-a6f6-221320d602cb",
   "metadata": {},
   "source": [
    "### 2.2 (5 pt)\n",
    "\n",
    "Ms. Sakota has assured you that both datasets contain questions of similar difficulty, so, what could be going on here?\n",
    "\n",
    "A. What is the distribution of correct answers (A, B, C, D) for each dataset? Create a bar chart to visualize this.\n",
    "\n",
    "B. Perform a chi-square test at $\\alpha = 0.05$, of independence to determine if there's a significant difference in the distribution of correct answers between the two datasets. What do you conclude?\n",
    "\n",
    "**hints**:\n",
    "- for (A), keep in mind that df_mmlu and df_other contain the results of all models, i.e., the `question_id` column is duplicated.\n",
    "- for (A), take care to clearly annotate the bar chart, e.g., title, y-label, legend.\n",
    "- for (B), clearly state the null hypothesis and alternative hypothesis\n",
    "- use the `chi2_contingency` function from `scipy.stats`\n",
    "- format your results from answer (A) as a 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b16f70-93e0-4a19-8a6d-b3ae5a75ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "fig, axs = plt.subplots(1,2,figsize=(15,5), sharey = True)\n",
    "\n",
    "# Count answers by letter\n",
    "df_counts_mmlu = df_mmlu.groupby(\"answer\").agg(\n",
    "    count=('answer', 'size'),\n",
    ").reset_index()\n",
    "df_counts_mmlu[\"frac_answer\"] = df_counts_mmlu[\"count\"] / len(df_mmlu.index) # Calculate the fraction of each answer\n",
    "display(df_counts_mmlu)\n",
    "\n",
    "# do the same for the orther df\n",
    "df_counts_other = df_other.groupby(\"answer\").agg(\n",
    "    count=('answer', 'size'),\n",
    ").reset_index()\n",
    "df_counts_other[\"frac_answer\"] = df_counts_other[\"count\"] / len(df_other.index)\n",
    "\n",
    "#mmlu dataset\n",
    "mmlu_data = df_accuracy[df_accuracy[\"dataset\"] == \"mmlu\"]\n",
    "axs[0].bar(df_counts_mmlu[\"answer\"],df_counts_mmlu[\"frac_answer\"], capsize=5, color='skyblue')\n",
    "axs[0].set_title('Distribution of correct answers - MMLU Dataset')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].set_xlabel('Answer Letter')\n",
    "axs[0].set_xticks(df_counts_mmlu[\"answer\"])\n",
    "axs[0].set_xticklabels(df_counts_mmlu[\"answer\"])\n",
    "axs[0].grid(axis='y')\n",
    "\n",
    "# Bar plot for df_other\n",
    "axs[1].bar(df_counts_other[\"answer\"], df_counts_other[\"frac_answer\"], capsize=5, color='salmon')\n",
    "axs[1].set_title('Distribution of Correct Answers - Other Dataset')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].set_xlabel('Answer Letter')\n",
    "axs[1].set_xticks(df_counts_other[\"answer\"])\n",
    "axs[1].set_xticklabels(df_counts_other[\"answer\"])\n",
    "axs[1].grid(axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92392cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B\n",
    "\n",
    "contingency_table = pd.DataFrame({\n",
    "    'Correct': [df_mmlu['correct'].sum(), df_other['correct'].sum()],\n",
    "    'Incorrect': [df_mmlu['correct'].count() - df_mmlu['correct'].sum(), \n",
    "                  df_other['correct'].count() - df_other['correct'].sum()]\n",
    "}, index=['MMLU', 'Other'])\n",
    "\n",
    "display(contingency_table)\n",
    "\n",
    "# Perform Chi-squared test\n",
    "from scipy.stats import chi2_contingency\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"Null Hypothesis (H0): Assumes that there is no association between the two categorical variables (i.e., there is a significant differce between the 2 distributions).\")\n",
    "print(\"Alternative Hypothesis (H1): Assumes that there is an association between the variables (i.e., they are similar).\")\n",
    "\n",
    "# Decision\n",
    "alpha = 0.05  # significance level\n",
    "if p < alpha:\n",
    "    conclusion = \"Reject the null hypothesis: There is a significant difference in the distribution of correct answers between the two datasets.\"\n",
    "else:\n",
    "    conclusion = \"Fail to reject the null hypothesis: There is no significant difference in the distribution of correct answers between the two datasets.\"\n",
    "\n",
    "print(conclusion)\n",
    "\n",
    "print(\"We conculde that even if we observe some differences in the percentage of correct answers between the 2 datasets, in reality they are just due to chances.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3125ad-5a7e-44e7-999a-a0ff99855d39",
   "metadata": {},
   "source": [
    "### 2.3 (7 pt)\n",
    "\n",
    "Let's dive in deeper:\n",
    "\n",
    "A. What is language model X's mean accuracy conditioned on the four answer options for each dataset?\n",
    "\n",
    "B. Compare LM X's performance when the correct answer is \"A\" between the two datasets. Use a T-test with CI = 0.95. What do you conclude?\n",
    "\n",
    "C. Compare LM X's performance when the correct answer is \"A\" vs. \"C or D\" for each dataset. Use a T-test with CI = 0.95. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39097a11-8efe-46d1-8bc3-5587bb58d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "df_X_mmlu = df_mmlu[df_mmlu[\"model_name\"] == \"X\"].groupby(\"answer\").agg(\n",
    "    tot_correct=(\"correct\", \"sum\"),\n",
    "    tot=(\"correct\", \"size\")\n",
    ")\n",
    "df_X_mmlu[\"accuracy\"] = df_X_mmlu[\"tot_correct\"] / df_X_mmlu[\"tot\"]\n",
    "display(df_X_mmlu)\n",
    "\n",
    "df_X_other = df_other[df_other[\"model_name\"] == \"X\"].groupby(\"answer\").agg(\n",
    "    tot_correct=(\"correct\", \"sum\"),\n",
    "    tot=(\"correct\", \"size\")\n",
    ")\n",
    "df_X_other[\"accuracy\"] = df_X_other[\"tot_correct\"] / df_X_other[\"tot\"]\n",
    "display(df_X_other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae67c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B\n",
    "\n",
    "# create arrays of right and wrong answers\n",
    "#mmlu\n",
    "mmlu_A = df_mmlu[(df_mmlu[\"model_name\"] == \"X\") & (df_mmlu[\"answer\"] == \"A\")][\"correct\"].to_list()\n",
    "mmlu_A = [int(x) for x in mmlu_A]\n",
    "\n",
    "# #other\n",
    "other_A = df_other[(df_other[\"model_name\"] == \"X\") & (df_other[\"answer\"] == \"A\")][\"correct\"].to_list()\n",
    "other_A = [int(x) for x in other_A]\n",
    "\n",
    "# perform t test\n",
    "t_A = ttest_ind(a = mmlu_A, b = other_A, equal_var=True) # uses 95% CI by deafualt\n",
    "\n",
    "# conclude\n",
    "\n",
    "print(\"Null Hypothesis: Mean of LM X's answers when the correct answer is A is the same between the two datasets.\")\n",
    "print(\"Alternative Hypothesis: Mean of LM X's answers when the correct answer is A is different between the two datasets.\")\n",
    "\n",
    "print(\"p value:\", round(t_A.pvalue, 3))\n",
    "\n",
    "print(\"The p value is greater than 0.05, therefore we do not reject the null hypothesis.\") \n",
    "print(\"We cannot say that there is a difference in LM X's performance between the two datasets when answering questions to which the correct answer is A.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636af6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C\n",
    "\n",
    "# create arrays of right and wrong answers\n",
    "# mmlu\n",
    "mmlu_CD_correct = df_X_mmlu.loc[['C', 'D'], 'tot_correct'].sum()\n",
    "mmlu_CD_tot = df_X_mmlu.loc[['C', 'D'], 'tot'].sum()\n",
    "mmlu_CD_incorrect = mmlu_CD_tot - mmlu_CD_correct\n",
    "\n",
    "mmlu_CD = [0] * mmlu_CD_incorrect + [1] * mmlu_CD_correct\n",
    "\n",
    "# other\n",
    "other_CD_correct = df_X_other.loc[['C', 'D'], 'tot_correct'].sum()\n",
    "other_CD_tot = df_X_other.loc[['C', 'D'], 'tot'].sum()\n",
    "other_CD_incorrect = other_CD_tot - other_CD_correct\n",
    "\n",
    "other_CD = [0] * other_CD_incorrect + [1] * other_CD_correct\n",
    "\n",
    "# perform t tests\n",
    "\n",
    "t_mmlu_ACD = ttest_ind(a = mmlu_A, b = mmlu_CD, equal_var=True)\n",
    "\n",
    "t_mmlu_other = ttest_ind(a = other_A, b = other_CD, equal_var=True)\n",
    "\n",
    "# conclude\n",
    "\n",
    "print(\"MMLU:\")\n",
    "print(\"Null Hypothesis: Mean of LM X's answers when the correct answer is A is the same as when it is C or D.\")\n",
    "print(\"Alternative Hypothesis: Mean of LM X's answers when the correct answer is A is different from when it is C or D.\")\n",
    "\n",
    "print(\"p value:\", t_mmlu_ACD.pvalue)\n",
    "\n",
    "print(\"The p value is smaller than 0.05, therefore we reject the null hypothesis.\") \n",
    "print(\"We can conclude that there is a difference in the model's performance when answering questions where A is correct and where C or D is correct.\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Other:\")\n",
    "\n",
    "print(\"p value:\", t_mmlu_other.pvalue)\n",
    "print(\"We can conclude the same thing for the Other dataset, the p value being under 0.05.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33848ff9-2604-4e48-b5df-3207dc81e9a9",
   "metadata": {},
   "source": [
    "### 2.4 (2 pt)\n",
    "\n",
    "What an intriguing finding! \n",
    "\n",
    "A. Print the mean accuracies conditioned on the correct answer for all LMs for each dataset.\n",
    "\n",
    "B. /Discuss:/ What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec53ce2c-866e-440a-ac79-7813cb756782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "\n",
    "models = df_mmlu[\"model_name\"].unique()  # Get unique model names from the MMLU dataset\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "accuracy_summary = pd.DataFrame()\n",
    "\n",
    "for model in models:\n",
    "    # MMLU Dataset\n",
    "    df_model_mmlu = df_mmlu[df_mmlu[\"model_name\"] == model].groupby(\"answer\").agg(\n",
    "        tot_correct=(\"correct\", \"sum\"),\n",
    "        tot=(\"correct\", \"size\")\n",
    "    )\n",
    "    df_model_mmlu[\"accuracy\"] = df_model_mmlu[\"tot_correct\"] / df_model_mmlu[\"tot\"]\n",
    "    df_model_mmlu[\"model_name\"] = model\n",
    "    df_model_mmlu[\"dataset\"] = \"mmlu\"\n",
    "    \n",
    "    # Append results to the summary DataFrame\n",
    "    accuracy_summary = pd.concat([accuracy_summary, df_model_mmlu.reset_index()], ignore_index=True)\n",
    "\n",
    "    # Other Dataset\n",
    "    df_model_other = df_other[df_other[\"model_name\"] == model].groupby(\"answer\").agg(\n",
    "        tot_correct=(\"correct\", \"sum\"),\n",
    "        tot=(\"correct\", \"size\")\n",
    "    )\n",
    "    df_model_other[\"accuracy\"] = df_model_other[\"tot_correct\"] / df_model_other[\"tot\"]\n",
    "    df_model_other[\"model_name\"] = model\n",
    "    df_model_other[\"dataset\"] = \"other\"\n",
    "    \n",
    "    # Append results to the summary DataFrame\n",
    "    accuracy_summary = pd.concat([accuracy_summary, df_model_other.reset_index()], ignore_index=True)\n",
    "\n",
    "# Display the summarized accuracy results for all models\n",
    "display(accuracy_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac101fa-090d-4a2d-a24f-faf45a172619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set the style for seaborn\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create a plot with facets for each dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a bar plot for the accuracy summary\n",
    "sns.barplot(data=accuracy_summary, x='answer', y='accuracy', hue='model_name', \n",
    "            palette='viridis', errorbar=None, dodge=True)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Mean Accuracy of Language Models Conditioned on Answer Options', fontsize=16)\n",
    "plt.ylabel('Mean Accuracy', fontsize=14)\n",
    "plt.xlabel('Answer Options', fontsize=14)\n",
    "plt.legend(title='Model Name')\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce58e47",
   "metadata": {},
   "source": [
    "B. /Discuss:/\n",
    "\n",
    "Model X is doing much better on questions to which the answer is A, compared to other answers. We can observe something similar for model Y and questions with answer D. model Z however seems to be doing quite badly on all questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd9d3c-5a03-4b30-84f7-a7dfbee1373e",
   "metadata": {},
   "source": [
    "### 2.5 (2 pt)\n",
    "\n",
    "Concerned with your findings so far, you quickly consult with Ms. Sakota. After thinking it over, Ms. Sakota concludes that more tests are needed. She orders a second round of MMLU results. However, the clever Ms. Sakota thinks of the following twist: while keeping questions fixed, she randomly permutes the position of the correct answer. The new results can be found in the folder `data/task_2_5/`:\n",
    "```\n",
    "task_2_5/\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ lm_scores_mmlu_shuffle.csv\n",
    "```\n",
    "\n",
    "/Discuss:/ Why would Ms. Sakota do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc586a",
   "metadata": {},
   "source": [
    "/Discuss:/\n",
    "\n",
    "For different pouposes, like:\n",
    "\n",
    "Avoid that the model becomes biased towards a specific answer location. For instance, if the correct answer is always the first one, the model will learn to pick the first one independantly of any correct explanation.\n",
    "Save time and money. This allows Ms. Sakota to create a \"new\" dataset for testing the LLM without having to spend much time or money."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9150ae0-dbaa-4c88-bf80-ec03127c6945",
   "metadata": {},
   "source": [
    "### 2.6 (4 pt)\n",
    "\n",
    "Increasingly sceptical of the language models' performance, you read up on proper testing practices. You stumble upon the concept of [test-rested stability](https://en.wikipedia.org/wiki/Repeatability), which roughtly states that:\n",
    "\n",
    "\"_Measurements taken by a single person or instrument on the same item, under the same conditions, and in a short period of time, should have the same results._\"\n",
    "\n",
    "In our case, we would assume an LM would have the same performance on a given question regardless of the correct answer position. One way of testing this is by using the following metric:\n",
    "\n",
    "$$\\text{test-retest metric} = \\frac{1}{N}\\sum_{i=1}^N \\frac{1}{M}\\sum_{j=1}^M c^i_0 c_j^i,$$\n",
    "\n",
    "where $c^i_0 \\in \\{0, 1\\}$ indicates whether the model answers the $i^{\\text{th}}$ question correctly (1 if correct, 0 if incorrect). $c_j^i$ indicates whether the model answers the $i^{\\text{th}}$ question correctly in the $j^{\\text{th}}$ shuffled version of the answer label content. Finally, $M$ is the total number of shuffles and $N$ is the dataset size.\n",
    "\n",
    "Task: compute the test-retest metric for each language model using the original `lm_scores_mmlu.csv` file and the new `lm_scores_mmlu_shuffle.csv` file. Using a bar plot, visualize your results by comparing the accuracy of the original `lm_scores_mmlu.csv` and the test-retest scores.\n",
    "\n",
    "**hints**\n",
    "- what is $M$ in our case?\n",
    "\n",
    "(bonus: no points, but so much sweet, sweet knowledge - check out [the following article](https://arxiv.org/pdf/2406.19470v1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c99df-88ff-4b74-89c5-6795d672f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new dataset\n",
    "df_mmlu = pd.read_csv('data/task_2/lm_scores_mmlu.csv')\n",
    "df_mmlu_shuffle = pd.read_csv(\"data/task_2_5/lm_scores_mmlu_shuffle.csv\")\n",
    "df_mmlu[\"shuffled\"] = True\n",
    "\n",
    "# merge them\n",
    "df = pd.merge(left = df_mmlu, \n",
    "              right = df_mmlu_shuffle,\n",
    "              on = [\"question_id\", \"model_name\"],\n",
    "              suffixes=[\"_normal\", \"_shuffle\"]\n",
    "              )\n",
    "\n",
    "print(df_mmlu.shape)\n",
    "print(df_mmlu_shuffle.shape)\n",
    "print(df.shape)\n",
    "df.head(3)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0287529-945f-4e03-9f57-2c9a90999ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy for the normal model\n",
    "df_accuracy = pd.DataFrame()\n",
    "\n",
    "models = df[\"model_name\"].unique()\n",
    "\n",
    "for model in models:\n",
    "    df_filtered = df_mmlu[df_mmlu[\"model_name\"] == model]\n",
    "    df_temp = pd.DataFrame({\n",
    "        \"accuracy\": [df_filtered[\"correct\"].sum()/df_filtered.shape[0]],\n",
    "        \"sem\": [df_filtered[\"correct\"].sem()],\n",
    "        \"model\": [model],\n",
    "        \"dataset\": [\"mmlu\"]\n",
    "    })\n",
    "    df_accuracy = pd.concat([df_accuracy, df_temp], ignore_index=True)\n",
    "\n",
    "display(df_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a55a00-3013-473e-be77-e40705737da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with M = 1 the formula should be:\n",
    "    # 1/n sum(c_mmlu * c_mmule_shuffled)\n",
    "\n",
    "trm = []\n",
    "for model in models:\n",
    "    df_filtered = df[df[\"model_name\"] == model]\n",
    "    # print(model)\n",
    "    # Calculate test retest metric\n",
    "    c_shuffle = df_filtered[\"correct_shuffle\"] #correct columns of shuffle\n",
    "    c_normal = df_filtered[\"correct_normal\"] #correct columns of normal\n",
    "    trm.append(np.mean(c_shuffle * c_normal))\n",
    "\n",
    "df_accuracy[\"trm\"] = trm\n",
    "\n",
    "display(df_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e1beb-a686-4972-a8dd-4c50027143c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the bar plot\n",
    "bar_width = 0.35  # Width of bars\n",
    "x = np.arange(len(df_accuracy['model']))  # the label locations\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create bars for accuracy and trm scores\n",
    "bars1 = ax.bar(x - bar_width/2, df_accuracy['accuracy'], bar_width, label='Original Accuracy', color='skyblue')\n",
    "bars2 = ax.bar(x + bar_width/2, df_accuracy['trm'], bar_width, label='Test-Retest Scores', color='salmon')\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Comparison of Original Accuracy and Test-Retest Scores')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_accuracy['model'])\n",
    "ax.legend()\n",
    "\n",
    "# Adding value labels on top of bars\n",
    "for bar in bars1:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 3), ha='center', va='bottom')\n",
    "\n",
    "for bar in bars2:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 3), ha='center', va='bottom')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7445820a-a273-43c9-899a-2463ec0e7432",
   "metadata": {},
   "source": [
    "discussion MAKE IT BETTER!!!\n",
    "\n",
    "Model's performance can vary significantly between tests.\n",
    "LM performs well on a training dataset but poorly on new, unseen data (as reflected by a low TRM), it could indicate that the model is overfitting.\n",
    "The test-retest scores are low for all three models, which confirms that the positioning of the answers makes quite a difference in the answer of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70bee6e-0c81-4f5a-b1a8-16a96aa2ae17",
   "metadata": {},
   "source": [
    "### 2.7 (2 pt)\n",
    "\n",
    "A. Using the unshuffled data: For each LM, print the distribution of the answers they give as well as the accuracy conditioned on the answer they give.\n",
    "\n",
    "B. /Discuss:/ Describe what you observe\n",
    "\n",
    "[bonus: not scored, but again _that sweet, sweet knowledge_] Could you think of a plausible explanation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4956581b-d047-46cb-ae71-1508a9a0b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "\n",
    "# MAYBE MAKE APLOT\n",
    "\n",
    "models = df_mmlu[\"model_name\"].unique()  # Get unique model names from the MMLU dataset\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "accuracy_summary = pd.DataFrame()\n",
    "\n",
    "for model in models:\n",
    "    # MMLU Dataset\n",
    "    df_model_mmlu = df_mmlu[df_mmlu[\"model_name\"] == model].groupby(\"result\").agg(\n",
    "        tot_correct=(\"correct\", \"sum\"),\n",
    "        tot=(\"correct\", \"size\")\n",
    "    )\n",
    "    df_model_mmlu[\"result_proportion\"] = df_model_mmlu[\"tot\"] / len(df_mmlu[df_mmlu[\"model_name\"] == model].index)\n",
    "    df_model_mmlu[\"accuracy\"] = df_model_mmlu[\"tot_correct\"] / df_model_mmlu[\"tot\"]\n",
    "    df_model_mmlu[\"model_name\"] = model\n",
    "    \n",
    "    # Append results to the summary DataFrame\n",
    "    accuracy_summary = pd.concat([accuracy_summary, df_model_mmlu.reset_index()], ignore_index=True)\n",
    "\n",
    "# Display the summarized accuracy results for all models\n",
    "display(accuracy_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65639228",
   "metadata": {},
   "source": [
    "B. /Discuss:/\n",
    "\n",
    "We can observe that both the accuracy and the proportion of results have a wide range of values depending on the result. This is particularily noticeable in models X and Y, X having accuracy ranging from 0.37 to 1.0 and Y having proportions ranging from 0.091 to 0.46.\n",
    "We can also note that answers with a low proportion like C and D for model X or A for model Y have the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3186fe-ef8e-4af3-9a07-a6081d454e5a",
   "metadata": {},
   "source": [
    "## Task 3 (16 points): What do Questions and Answers look like for a Language Model?\n",
    "\n",
    "While you feel pretty good about the tests you conducted so far, something still bothers you: what if the language models don't see the data like you do? Suddenly, you receive a phone call from a wise AI sage in the West, _Westoda_:\n",
    "\n",
    "```\n",
    "\"Hmm, correct you are, young padawan, to question how the world is seen by large language models! Simple 'text' it is not, hmm? No, no, no! Characters and words, the way of puny humans, this is not, heh heh heh.\n",
    "\n",
    "'Tokens', they use, yes! Mysterious and powerful, these tokens are. Expand our vocabulary, they do, beyond the simple 'a to Z'. Chunky blocks of text, they become, yes! 'Hello world', a simple phrase it may seem. But to a language model, '[24912, 2375]' it might appear, yes! Confusing, it is, hmm?\n",
    "\n",
    "Wise, it would be, to explore these MMLU data points through the eyes of a language model, you think? Yes, yes! Much to learn, there is. The ways of the tokens, understand you must, if truly comprehend the great LMs, you wish to.\n",
    "Meditate on this, you should. The force of natural language processing, strong it is. But patience, you must have, my young padawan. For only through great study and contemplation, will the mysteries of the tokens reveal themselves to you, they will. Yes, hmmm!\"\n",
    "```\n",
    "\n",
    "Admittingly, Westoda at times speaks in riddles‚Ä¶ However, he was explaining a crucial aspect of modern LMs called [Tokenization](https://learn.microsoft.com/en-us/dotnet/ai/conceptual/understanding-tokens):\n",
    "\n",
    "\n",
    "‚ÄúTokens are words, character sets, or combinations of words and punctuation that are used by [language models (LMs)] to decompose text into. Tokenization is the first step in training‚Äù\n",
    "\n",
    "Instead of characters, LMs process natural language using ‚Äútokens‚Äù. While this is useful for a number of reasons, it does at times introduce some ‚Äúunintuitive‚Äù behavior‚Ä¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c66517-938b-4331-9eea-1b23fe4ad9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROVIDED CODE\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "except Exception as e:\n",
    "    print('installing tiktoken package')\n",
    "    \n",
    "    !pip install tiktoken\n",
    "    \n",
    "    import tiktoken\n",
    "\n",
    "def tokenize_text(s):\n",
    "    enc = tiktoken.encoding_for_model('gpt-4o')\n",
    "    tokens = enc.encode(str(s))\n",
    "    return tokens\n",
    "\n",
    "example_string = 'hello world'\n",
    "print(f'humans see: \"{example_string}\" --> language models see: {tokenize_text(example_string)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8019ee-7d52-496f-afff-c96f2f9db08c",
   "metadata": {},
   "source": [
    "### 3.1 (5 pt)\n",
    "\n",
    "Use the provided code in the cell above to \"see the world through the eyes of a language model\":\n",
    "\n",
    "A. Tokenize the questions of the original MMLU data provided in task 1: `task_1/mmlu_data/test.csv` and plot the token distribution (the frequency of each token).\n",
    "\n",
    "B. Same as (A), but now for the answers in columns (columns \"A\", \"B\", \"C\", and \"D\").\n",
    "\n",
    "C. Isolate the tokens for the strings \"A\", \"B\", \"C\", and \"D\", then, for their occurances in both questions and answers, print their relative distribution to each other.\n",
    "\n",
    "**hint**\n",
    "- There are a _lot_ of tokens, consider using a cutoff point and log scale\n",
    "- For (c), they should sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1e97b-3a31-41b1-a9c3-fb84cb1740d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "\n",
    "df_test_tokenized = df_test.copy()\n",
    "\n",
    "df_test_tokenized[\"tokenized_question\"] = df_test_tokenized[\"question\"].apply(lambda s : tokenize_text(s))\n",
    "\n",
    "df_test_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5446257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tokens(token_series: pd.Series) -> pd.Series:\n",
    "    # concatenate all tokens in the series\n",
    "    all_tokens = []\n",
    "    for tokens in token_series:\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    # create a series from the list of tokens\n",
    "    return pd.Series(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da26757-ddce-4e51-b723-6d85dfb82e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times each token appears\n",
    "sr_question_tokens = get_all_tokens(df_test_tokenized[\"tokenized_question\"])\n",
    "\n",
    "# find the percentiles of the tokens\n",
    "percentiles = np.arange(0, 1.01, 0.05)\n",
    "token_percentiles = sr_question_tokens.quantile(percentiles)\n",
    "\n",
    "# count the number of unique tokens in each percentile\n",
    "count_unique_tokens = token_percentiles.apply(\n",
    "    lambda x: sr_question_tokens[sr_question_tokens <= x].nunique()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "count_unique_tokens.plot(logy=True)\n",
    "\n",
    "plt.title(\"Percentiles of Tokens in Original MMLU Data\")\n",
    "plt.ylabel(\"Number of unique tokens\")\n",
    "plt.xlabel(\"Percentile\")\n",
    "plt.xticks(percentiles)\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a9bf9-1b35-418b-975f-b79bc460bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats about the cut-off point\n",
    "cut_off_point = int(sr_question_tokens.quantile(0.3))\n",
    "print(\"Cut-off point:\", cut_off_point)\n",
    "sr_question_tokens_cut = sr_question_tokens[sr_question_tokens <= cut_off_point]\n",
    "print(\"Number unique of tokens <= cut-off point:\", sr_question_tokens_cut.nunique())\n",
    "print(\"Number of tokens <= cut-off point:\", len(sr_question_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "952f13eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the distribution is right-skewed and the number of unique tokens starts increasing significantly after 0.3.\n",
    "# As the point still keeps a reasonable amount of unique tokens, we choose 30th percentile as the cut-off point.\n",
    "sr_question_tokens_cut_freq = sr_question_tokens_cut.value_counts() / len(sr_question_tokens_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff6286-6de7-4eb5-b879-235947d564ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the token distribution\n",
    "\n",
    "plt.figure(figsize=(35, 10))\n",
    "\n",
    "sr_question_tokens_cut_freq.plot(kind=\"bar\", logy=True)\n",
    "\n",
    "plt.title(\n",
    "    f\"Question Token Distribution of Original MMLU Data (token <= {cut_off_point})\",\n",
    "    fontsize=22,\n",
    ")\n",
    "plt.ylabel(\"Frequency\", fontsize=20)\n",
    "plt.xlabel(\"Token\", fontsize=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B\n",
    "\n",
    "# tokenize the answers\n",
    "df_test_tokenized[\"tokenized_A\"] = df_test_tokenized[\"A\"].apply(lambda s : tokenize_text(s))\n",
    "df_test_tokenized[\"tokenized_B\"] = df_test_tokenized[\"B\"].apply(lambda s : tokenize_text(s))\n",
    "df_test_tokenized[\"tokenized_C\"] = df_test_tokenized[\"C\"].apply(lambda s : tokenize_text(s))\n",
    "df_test_tokenized[\"tokenized_D\"] = df_test_tokenized[\"D\"].apply(lambda s : tokenize_text(s))\n",
    "\n",
    "df_test_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c609fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times each token appears\n",
    "# We use the same cut-off point as before to keep the same range of unique tokens\n",
    "\n",
    "sr_A_tokens = get_all_tokens(df_test_tokenized[\"tokenized_A\"])\n",
    "sr_A_tokens_cut = sr_A_tokens[sr_A_tokens <= cut_off_point]\n",
    "sr_A_token_counts_cut = sr_A_tokens_cut.value_counts()\n",
    "sr_A_token_counts_cut_freq = sr_A_token_counts_cut / len(sr_A_tokens_cut)\n",
    "\n",
    "sr_B_tokens = get_all_tokens(df_test_tokenized[\"tokenized_B\"])\n",
    "sr_B_tokens_cut = sr_B_tokens[sr_B_tokens <= cut_off_point]\n",
    "sr_B_token_counts_cut = sr_B_tokens_cut.value_counts()\n",
    "sr_B_token_counts_cut_freq = sr_B_token_counts_cut / len(sr_B_tokens_cut)\n",
    "\n",
    "sr_C_tokens = get_all_tokens(df_test_tokenized[\"tokenized_C\"])\n",
    "sr_C_tokens_cut = sr_C_tokens[sr_C_tokens <= cut_off_point]\n",
    "sr_C_token_counts_cut = sr_C_tokens_cut.value_counts()\n",
    "sr_C_token_counts_cut_freq = sr_C_token_counts_cut / len(sr_C_tokens_cut)\n",
    "\n",
    "sr_D_tokens = get_all_tokens(df_test_tokenized[\"tokenized_D\"])\n",
    "sr_D_tokens_cut = sr_D_tokens[sr_D_tokens <= cut_off_point]\n",
    "sr_D_token_counts_cut = sr_D_tokens_cut.value_counts()\n",
    "sr_D_token_counts_cut_freq = sr_D_token_counts_cut / len(sr_D_tokens_cut)\n",
    "\n",
    "dict_answer_token_cut_counts = {\n",
    "    \"A\": sr_A_token_counts_cut_freq,\n",
    "    \"B\": sr_B_token_counts_cut_freq,\n",
    "    \"C\": sr_C_token_counts_cut_freq,\n",
    "    \"D\": sr_D_token_counts_cut_freq,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bba198-e1e1-4aa2-ad0b-2c7b5dcb37d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the token distribution\n",
    "\n",
    "for answer, freq in dict_answer_token_cut_counts.items():\n",
    "\n",
    "    plt.figure(figsize=(35, 10))\n",
    "\n",
    "    freq.plot(kind=\"bar\", logy=True)\n",
    "\n",
    "    plt.title(\n",
    "        f\"Answer {answer} Token Distribution of Original MMLU Data (token <= {cut_off_point})\",\n",
    "        fontsize=22,\n",
    "    )\n",
    "    plt.ylabel(\"Frequency\", fontsize=20)\n",
    "    plt.xlabel(\"Token\", fontsize=20)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd392df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C\n",
    "\n",
    "# build a dictionary of \"A\", \"B\", \"C\", \"D\" tokens\n",
    "iso_chars = [\"A\", \"B\", \"C\", \"D\"]\n",
    "dict_ABCD_token = {k: tokenize_text(k)[0] for k in iso_chars}\n",
    "\n",
    "# find the occurances of \"A\", \"B\", \"C\", \"D\" in the questions and answers\n",
    "for char, token in dict_ABCD_token.items():\n",
    "    df_test_tokenized[f\"{char}_occur_pair\"] = df_test_tokenized[\n",
    "        \"tokenized_question\"\n",
    "    ].apply(lambda x: token in x) | df_test_tokenized[f\"tokenized_{char}\"].apply(\n",
    "        lambda x: token in x\n",
    "    )\n",
    "\n",
    "# count the number of occurances\n",
    "df_chars = pd.DataFrame.from_dict(dict_ABCD_token, orient=\"index\").reset_index()\n",
    "df_chars.columns = [\"char\", \"token\"]\n",
    "df_chars[\"count_occur_question_answers\"] = df_chars[\"char\"].apply(\n",
    "    lambda x: df_test_tokenized[f\"{x}_occur_pair\"].sum()\n",
    ")\n",
    "\n",
    "# compute the relative distribution\n",
    "df_chars[\"count_occur_question_answers_rel\"] = (\n",
    "    df_chars[\"count_occur_question_answers\"]\n",
    "    / df_chars[\"count_occur_question_answers\"].sum()\n",
    ")\n",
    "\n",
    "print(\"Relative distribution of A, B, C, D tokens occuring in both the questions and answers:\")\n",
    "display(df_chars[[\"char\", \"count_occur_question_answers_rel\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a674929c-68e1-4cd8-97b3-efa0cdae4874",
   "metadata": {},
   "source": [
    "### 3.2 (3 pt)\n",
    "\n",
    "What if the number of \"A\", \"B\", \"C\", and \"D\" tokens in the question and answer pairs could influence a language model's decisions?\n",
    "\n",
    "A. For each combined question-answers pair, compute: \n",
    "1. the number of \"A\", \"B\", \"C\", and \"D\" tokens; and\n",
    "2. the total number of tokens.\n",
    "3. then, group by the \"correct\" answer and compute the mean frequency of A, B, C, and D tokens and the total number of tokens. \n",
    "4. finally, print your results\n",
    "\n",
    "B. /Discuss:/ What do you think of the hypothesis that the frequency of A, B, C, and D tokens could influence answers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25419cc1-f058-4c51-a0aa-577272b8b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "\n",
    "new_col_name_total = \"token_total\"\n",
    "\n",
    "# count the number of total tokens in the question\n",
    "df_test_tokenized[new_col_name_total] = df_test_tokenized[\n",
    "    \"tokenized_question\"\n",
    "].apply(len)\n",
    "\n",
    "# compute number of tokens in question-answer pairs\n",
    "for char, token in dict_ABCD_token.items():\n",
    "    new_col_name_count = f\"token_{char}_count\"\n",
    "    # count the number of times the token appears in the question\n",
    "    df_test_tokenized[new_col_name_count] = df_test_tokenized[\n",
    "        \"tokenized_question\"\n",
    "    ].apply(lambda x: x.count(token))\n",
    "\n",
    "    for c in iso_chars:\n",
    "        # add the number of times the token appears in the answers in new_col_name_count\n",
    "        df_test_tokenized[new_col_name_count] += df_test_tokenized[\n",
    "            f\"tokenized_{c}\"\n",
    "        ].apply(lambda x: x.count(token))\n",
    "\n",
    "        # add the number of total tokens in the answers in new_col_name_total\n",
    "        df_test_tokenized[new_col_name_total] += df_test_tokenized[\n",
    "            f\"tokenized_{c}\"\n",
    "        ].apply(len)\n",
    "\n",
    "df_test_tokenized.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "32cd0ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by answer\n",
    "group_by_answer = df_test_tokenized.groupby(\"answer\").agg(\n",
    "    mean_freq_char_A = (\"token_A_count\", \"mean\"),\n",
    "    mean_freq_char_B = (\"token_B_count\", \"mean\"),\n",
    "    mean_freq_char_C = (\"token_C_count\", \"mean\"),\n",
    "    mean_freq_char_D = (\"token_D_count\", \"mean\"),\n",
    "    mean_freq_total_pair = (\"token_total\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db03aa-48b1-46bd-9494-655542d4ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea8a279",
   "metadata": {},
   "source": [
    "B. /Discuss:/\n",
    "\n",
    "The mean frequency of each letter corresponding to the answers is very close. Besides, the frequency of the letter \"A\" is noticeably higher than other letters across all answers provided by the model. We believe that individual letters do not influence the model's choice of answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25ef95-d2ce-4112-87f7-8b0a52755e2a",
   "metadata": {},
   "source": [
    "### 3.3 (4 pt)\n",
    "\n",
    "Three of the most important considerations when deciding between language models are:\n",
    "\n",
    "Quality\n",
    "Costs\n",
    "Speed\n",
    "\n",
    "So far, much of your analysis has focused on quality. However, the government has indicated that they are quite concerned about both the total costs and speed as well. Specifically, it has been brought to their attention that a new `turbo` model has been launched! \n",
    "\n",
    "This model is both cheaper and faster than the models you evaluated so far. However, there is a catch: the context length* is much smaller than that of the other LMS. Namely, it can only process **300** tokens during inference. Meanwhile, the other models can process up to 100K tokens! \n",
    "\n",
    "*_The ‚Äúcontext length‚Äù refers to the number of tokens that can be given to an LM as input._\n",
    "\n",
    "A. Are there subjects where using the cheaper model might be problematic? I.e., where part of the question and answer(s) might not fit completely in the context?\n",
    "\n",
    "B. /Discuss:/ Can you think of a strategy that would balance the needs of the government?\n",
    "\n",
    "**hint**:\n",
    "- An LM needs to have both the question and the different answer options in its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a6fa3-2735-44d7-9944-076cc9f679ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "\n",
    "# find context that exceeds 300 tokens\n",
    "# original_count --> total number of questions (of that category)\n",
    "# count --> total number of questions (of that category) that have more than 300 tokens (counting both qeustion and answers)\n",
    "df_test_tokenized[\"context\"] = (\n",
    "    df_test_tokenized[\"tokenized_question\"]\n",
    "    + df_test_tokenized[\"tokenized_A\"]\n",
    "    + df_test_tokenized[\"tokenized_B\"]\n",
    "    + df_test_tokenized[\"tokenized_C\"]\n",
    "    + df_test_tokenized[\"tokenized_D\"]\n",
    ")\n",
    "\n",
    "# find questions that exceed 300 tokens\n",
    "df_token_exceeded = df_test_tokenized[df_test_tokenized[\"context\"].apply(len) > 300]\n",
    "\n",
    "# calculate the number of subjects that exceed 300 tokens\n",
    "# columns: subject, count\n",
    "df_subject_exceeded = df_token_exceeded[\"subject\"].value_counts().reset_index()\n",
    "\n",
    "# calculate the total number of questions (of that category)\n",
    "df_subject_exceeded[\"original_count\"] = df_subject_exceeded[\"subject\"].apply(\n",
    "    lambda x: df_test_tokenized[df_test_tokenized[\"subject\"] == x].shape[0]\n",
    ")\n",
    "\n",
    "# calculate the rate\n",
    "df_subject_exceeded[\"rate\"] = (\n",
    "    df_subject_exceeded[\"count\"] / df_subject_exceeded[\"original_count\"]\n",
    ")\n",
    "df_subject_exceeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c2a9f-befc-4fab-9405-dfd722e94bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find problematic subjects and that significantly affected\n",
    "problemal_subject = df_subject_exceeded[\"subject\"]\n",
    "significant_subject = df_subject_exceeded[df_subject_exceeded[\"rate\"] > 0.1][\"subject\"]\n",
    "print(\"Affected subjects:\")\n",
    "for subject in problemal_subject:\n",
    "    print(\"\\t\", subject)\n",
    "print(\"Problematic subjects (significantly affected, with rate > 0.1):\")\n",
    "for subject in significant_subject:\n",
    "    print(\"\\t\", subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8a44a",
   "metadata": {},
   "source": [
    "B. /Dicsuss:/\n",
    "\n",
    "Use the original model in subjects that are significantly affected, such as law and history, and apply the new turbo model in other subjects.\n",
    "\n",
    "ry to summarize adn re-formulate the questions (like remove reuse of part of the question in the answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f07bf-558f-467c-8f66-d88db561d455",
   "metadata": {},
   "source": [
    "### 3.4 (4 pt)\n",
    "\n",
    "/Discuss:/ The time has come to give your final recommendation on the use of LMs in education to the government! Taking into account everything you analyzed in all the preceding tasks (1, 2, and 3), please write a short recommendation consisting of 4 bullet points discussing your concerns.\n",
    "\n",
    "**hint**\n",
    "- Try to use the MECE framework: _Mutually Exclusive Collectively Exhaustive_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a5fa5",
   "metadata": {},
   "source": [
    "/Discuss:/\n",
    "\n",
    "1. The model exhibits overfitting, performing worse on other datasets compared to the MMLU dataset. This suggests that the model can carry inherent biases from their training data, and its accuracy in real-world use may not meet expected performance.\n",
    "\n",
    "2. The model is sensitive to the positional information of the answers, meaning that when asking questions, the model shows bias in answer ranking and may not treat all answers impartially.\n",
    "\n",
    "3. In subjects with longer contexts, such as history and law, where the text is more extensive, the model may lack the ability to process all the information, resulting in an inability to effectively address issues in these subjects.\n",
    "\n",
    "4. The use of the model may incur significant costs, and its application across subjects should be adjusted based on budget considerations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
